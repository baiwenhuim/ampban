{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba6b0d51-b63a-4862-819e-e1a8c2009640",
   "metadata": {},
   "source": [
    "## AMPBAN\n",
    "Here is the detailed description for AMPBAN model, from the building of benchmark dataset, indepdendent test dataset, build model to comparision with SOTA AMP prediction model;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8cb35ec-4a44-4a90-8cd0-c58fa8369de4",
   "metadata": {},
   "source": [
    "### Benchmark dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "436aa48d-884f-474e-9d5f-0b1d10f77fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import SeqIO\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed1adf2e-321e-4e8b-9d3f-9e2e2aa8545a",
   "metadata": {},
   "outputs": [],
   "source": [
    "peptipedia_id = pd.read_csv('./data/peptipedia/activities_canon_AMP-20241204.csv', header=0)['id_sequence'].to_list()\n",
    "peptipedia = pd.read_csv('./data/peptipedia/activities_canon_AMP-20241204.csv', header=0)['sequence'].to_list()\n",
    "peptipedia_5_100 = [seq for seq in peptipedia if 5 <= len(seq) <= 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a8a4db9-fe4c-4f23-be62-3bfb385b6114",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32846"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(peptipedia_5_100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1058d274-ccf3-4fff-becb-54e837557075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输入文件中总共有 58095 条序列\n",
      "Number of unique sequences: 43913\n",
      "Deduplicated FASTA saved to temp.fasta\n",
      "Filtered 42163 negative samples saved to ./data/Swiss-Prot/negative_processed_samples.fasta\n"
     ]
    }
   ],
   "source": [
    "# Input and output file paths\n",
    "input_file = \"./data/Swiss-Prot/uniprotkb_reviewed_true_AND_length_5_TO_2024_12_26.fasta\"\n",
    "temp_file = \"temp.fasta\"\n",
    "output_file = \"./data/Swiss-Prot/negative_processed_samples.fasta\"\n",
    "\n",
    "# Keywords to exclude antimicrobial, antibiotic, antibacterial, antiviral, antifungal, antimalarial, antiparasitic,\n",
    "# anti-protist, anticancer, defense, defensin, cathelicidin, histatin, bacteriocin, microbicidal, fungicide\n",
    "keywords = [\"antimicrobial\", \"antibacterial\", \"antifungal\", \"anticancer\", \"antimalarial\", \"anti-protist\", \n",
    "            \"antiviral\", \"antiparasitic\", \"antibiotic\", \"antibiofilm\", \"defense\", \"defensin\", \"cathelicidin\", \n",
    "            \"effector\", \"excreted\", \"bacteriocin\", \"microbicidal\", \"microbicidal\", \"histatin\"]\n",
    "\n",
    "# Non-standard residues\n",
    "non_standard_residues = set(\"BJOUXZ\")\n",
    "\n",
    "def deduplicate_fasta(input_file, output_file):\n",
    "    \"\"\"\n",
    "    Remove duplicate sequences from a FASTA file.\n",
    "\n",
    "    Args:\n",
    "        input_file (str): Path to the input FASTA file.\n",
    "        output_file (str): Path to save the deduplicated FASTA file.\n",
    "    \"\"\"\n",
    "    seen_sequences = set()  # To track unique sequences\n",
    "    unique_records = []  # To store unique SeqRecord objects\n",
    "    total_count = 0\n",
    "\n",
    "    # Read the FASTA file\n",
    "    for record in SeqIO.parse(input_file, \"fasta\"):\n",
    "        total_count += 1\n",
    "        seq = str(record.seq)  # Convert sequence to string\n",
    "        if seq not in seen_sequences:\n",
    "            seen_sequences.add(seq)  # Add sequence to the seen set\n",
    "            unique_records.append(record)  # Keep the record\n",
    "\n",
    "    # Report statistics\n",
    "    print(f\"输入文件中总共有 {total_count} 条序列\")\n",
    "    total_sequences = len(unique_records)\n",
    "    print(f\"Number of unique sequences: {total_sequences}\")\n",
    "    # Write the deduplicated sequences to the output file\n",
    "    SeqIO.write(unique_records, output_file, \"fasta\")\n",
    "    print(f\"Deduplicated FASTA saved to {output_file}\")\n",
    "\n",
    "deduplicate_fasta(input_file, temp_file)\n",
    "\n",
    "# Function to filter sequences\n",
    "def is_negative_sample(record):\n",
    "    # Check for keywords\n",
    "    if any(keyword in record.description.lower() for keyword in keywords):\n",
    "        return False\n",
    "    # Check sequence length\n",
    "    if not (5 <= len(record.seq) <= 100):\n",
    "        return False\n",
    "    # Check for non-standard residues\n",
    "    if any(residue in non_standard_residues for residue in record.seq):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "# Filter sequences\n",
    "negative_samples = [record for record in SeqIO.parse(temp_file, \"fasta\") if is_negative_sample(record)]\n",
    "\n",
    "# Write filtered sequences to output file\n",
    "SeqIO.write(negative_samples, output_file, \"fasta\")\n",
    "os.remove(\"temp.fasta\")\n",
    "print(f\"Filtered {len(negative_samples)} negative samples saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eed7c368-3c3e-49c3-90cc-8cd1449be84c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating positive samples from ./data/peptipedia/activities_canon_AMP-20241204.csv...\n",
      "Generated 32846 positive samples. Saved to ./data/peptipedia/positive_samples.fasta\n",
      "Combining positive samples (./data/peptipedia/positive_samples.fasta) and negative samples (./data/Swiss-Prot/negative_processed_samples.fasta)...\n",
      "Combined training data saved to ./data/combined_training_data.fasta\n",
      "Loaded 1221 sequences from test file: ./data/independent_test_data/MFA_independent.fasta\n",
      "Loaded 1756 sequences from test file: ./data/independent_test_data/xiao_independent_new.fasta\n",
      "Loaded 3072 sequences from test file: ./data/independent_test_data/xu_independent.fasta\n",
      "Total sequences in combined training data: 75009\n",
      "\n",
      "--- Filtering Statistics ---\n",
      "Total sequences before filtering: 75009\n",
      "Positive sequences (with '_1') before filtering: 32846\n",
      "Negative sequences (with '_0') before filtering: 42163\n",
      "--------------------------------------------------\n",
      "Unique sequences saved to ./data/training_data.fasta: 69646\n",
      "Positive sequences (with '_1') after filtering: 30971\n",
      "Negative sequences (with '_0') after filtering: 38675\n",
      "\n",
      "Process completed successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from Bio import SeqIO\n",
    "from Bio.SeqRecord import SeqRecord\n",
    "from Bio.Seq import Seq\n",
    "import os\n",
    "\n",
    "def generate_positive_samples(csv_path, output_fasta_path):\n",
    "    \"\"\"\n",
    "    Generates a FASTA file of positive samples from a CSV.\n",
    "    \"\"\"\n",
    "    print(f\"Generating positive samples from {csv_path}...\")\n",
    "    df = pd.read_csv(csv_path)\n",
    "    \n",
    "    if 'id_sequence' not in df.columns or 'sequence' not in df.columns:\n",
    "        raise ValueError(\"CSV file must contain 'id_sequence' and 'sequence' columns.\")\n",
    "    \n",
    "    filtered_df = df[(df['sequence'].str.len() >= 5) & (df['sequence'].str.len() <= 100)]\n",
    "    \n",
    "    with open(output_fasta_path, \"w\") as f:\n",
    "        for _, row in filtered_df.iterrows():\n",
    "            f.write(f\">{row['id_sequence']}\\n{row['sequence']}\\n\")\n",
    "    \n",
    "    print(f\"Generated {len(filtered_df)} positive samples. Saved to {output_fasta_path}\")\n",
    "    return output_fasta_path\n",
    "\n",
    "def load_sequences(file_path):\n",
    "    \"\"\"Load sequences from a FASTA file into a set for quick lookup.\"\"\"\n",
    "    return {str(record.seq) for record in SeqIO.parse(file_path, \"fasta\")}\n",
    "\n",
    "def load_sequences_with_ids(file_path):\n",
    "    \"\"\"Load sequences from a FASTA file into a list of (id, sequence) tuples, preserving order.\"\"\"\n",
    "    return [(record.id, str(record.seq)) for record in SeqIO.parse(file_path, \"fasta\")]\n",
    "\n",
    "# 【核心修改处】在合并时为ID添加 _1 / _0 后缀\n",
    "def combine_positive_and_negative(positive_file, negative_file, combined_output_file):\n",
    "    \"\"\"\n",
    "    Combines positive and negative samples into a single FASTA file.\n",
    "    Appends '_1' to positive IDs and '_0' to negative IDs.\n",
    "    \"\"\"\n",
    "    print(f\"Combining positive samples ({positive_file}) and negative samples ({negative_file})...\")\n",
    "    \n",
    "    with open(combined_output_file, \"w\") as out_f:\n",
    "        # 处理阳性序列，添加 _1 后缀\n",
    "        for record in SeqIO.parse(positive_file, \"fasta\"):\n",
    "            modified_id = f\"{record.id}_1\"  # 关键：为阳性ID添加 _1\n",
    "            new_record = SeqRecord(Seq(record.seq), id=modified_id, description=\"\")\n",
    "            SeqIO.write(new_record, out_f, \"fasta\")\n",
    "        \n",
    "        # 处理阴性序列，添加 _0 后缀\n",
    "        for record in SeqIO.parse(negative_file, \"fasta\"):\n",
    "            modified_id = f\"{record.id}_0\"  # 关键：为阴性ID添加 _0\n",
    "            new_record = SeqRecord(Seq(record.seq), id=modified_id, description=\"\")\n",
    "            SeqIO.write(new_record, out_f, \"fasta\")\n",
    "    \n",
    "    print(f\"Combined training data saved to {combined_output_file}\")\n",
    "    return combined_output_file\n",
    "\n",
    "def filter_unique_sequences(test_files, train_file, output_file):\n",
    "    \"\"\"\n",
    "    Removes sequences from the training file that exist in any of the test files.\n",
    "    \"\"\"\n",
    "    all_test_sequences = set()\n",
    "    for test_file in test_files:\n",
    "        test_seqs = load_sequences(test_file)\n",
    "        all_test_sequences.update(test_seqs)\n",
    "        print(f\"Loaded {len(test_seqs)} sequences from test file: {test_file}\")\n",
    "    \n",
    "    train_sequences = load_sequences_with_ids(train_file)\n",
    "    total_train_sequences = len(train_sequences)\n",
    "    print(f\"Total sequences in combined training data: {total_train_sequences}\")\n",
    "    \n",
    "    # 过滤前统计（现在ID已带 _1/_0，统计会生效）\n",
    "    count_true_before = sum(1 for seq_id, _ in train_sequences \n",
    "                           if \"_1\" in seq_id or \"true\" in seq_id.lower())\n",
    "    count_false_before = sum(1 for seq_id, _ in train_sequences \n",
    "                            if \"_0\" in seq_id or \"false\" in seq_id.lower())\n",
    "    \n",
    "    # 过滤重复序列\n",
    "    unique_train = []\n",
    "    count_true = 0\n",
    "    count_false = 0\n",
    "    \n",
    "    for seq_id, seq in train_sequences:\n",
    "        if seq not in all_test_sequences:\n",
    "            unique_train.append((seq_id, seq))\n",
    "            \n",
    "            # 过滤后统计（同样依赖 _1/_0 后缀）\n",
    "            if \"_1\" in seq_id or \"true\" in seq_id.lower():\n",
    "                count_true += 1\n",
    "            elif \"_0\" in seq_id or \"false\" in seq_id.lower():\n",
    "                count_false += 1\n",
    "    \n",
    "    # 保存过滤后的训练集（ID已带 _1/_0，可直接用于模型训练）\n",
    "    with open(output_file, \"w\") as f:\n",
    "        for seq_id, seq in unique_train:\n",
    "            f.write(f\">{seq_id}\\n{seq}\\n\")\n",
    "    \n",
    "    # 输出正确的统计结果\n",
    "    print(\"\\n--- Filtering Statistics ---\")\n",
    "    print(f\"Total sequences before filtering: {total_train_sequences}\")\n",
    "    print(f\"Positive sequences (with '_1') before filtering: {count_true_before}\")\n",
    "    print(f\"Negative sequences (with '_0') before filtering: {count_false_before}\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"Unique sequences saved to {output_file}: {len(unique_train)}\")\n",
    "    print(f\"Positive sequences (with '_1') after filtering: {count_true}\")\n",
    "    print(f\"Negative sequences (with '_0') after filtering: {count_false}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 路径配置（根据你的实际路径调整）\n",
    "    positive_csv = './data/peptipedia/activities_canon_AMP-20241204.csv'\n",
    "    negative_fasta = './data/Swiss-Prot/negative_processed_samples.fasta'\n",
    "    test_files = [\n",
    "        './data/independent_test_data/MFA_independent.fasta',\n",
    "        './data/independent_test_data/xiao_independent_new.fasta',\n",
    "        './data/independent_test_data/xu_independent.fasta'\n",
    "    ]\n",
    "    output_file = './data/training_data.fasta'\n",
    "\n",
    "    # 步骤1：从CSV生成阳性序列FASTA（无后缀）\n",
    "    positive_fasta = generate_positive_samples(positive_csv, \"./data/peptipedia/positive_samples.fasta\")\n",
    "    \n",
    "    # 步骤2：合并阳/阴性序列，并添加 _1/_0 后缀（核心修复步骤）\n",
    "    combined_train_file = \"./data/combined_training_data.fasta\"\n",
    "    combine_positive_and_negative(positive_fasta, negative_fasta, combined_train_file)\n",
    "    \n",
    "    # 步骤3：过滤训练集中与测试集重复的序列\n",
    "    filter_unique_sequences(test_files, combined_train_file, output_file)\n",
    "    \n",
    "    print(\"\\nProcess completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16762968-92ca-4d47-bbe7-2e6b476a4bec",
   "metadata": {},
   "source": [
    "### Extract features for training data and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5e633e-c2a2-4e24-bacc-8ff3daea2623",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First step: Predict structure for peptides\n",
    "# The core script predict_structure_ESMfold.py was embedd in below shell script, change directory path for different datasets\n",
    "# Which may need run in another environment \"conda activate django\"\n",
    "!bash ./script/predict_all_fasta_ESMFold_individual_folders.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f231a23-6158-4018-9f79-907f3ed3bf0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second step: Extract structure feature with Progres\n",
    "# The core script generate_egnn_fea.py was embedd in below shell script, change directory path for different datasets\n",
    "# Which may need run in another environment \"conda activate prog\"\n",
    "!bash ./script/generate_stru_fea_indep.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0260a62-0bf1-4172-b6e9-dc342a6b2daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The structure embedding of training data and test data were below\n",
    "/mnt/e/Doctorale/Project/AMPBAN/data/protein_egnn_embeddings.pt\n",
    "/mnt/e/Doctorale/Project/AMPBAN/data/independent_test_data/xu_independent_structure.pt\n",
    "/mnt/e/Doctorale/Project/AMPBAN/data/independent_test_data/xiao_independent_new_stru.pt\n",
    "/mnt/e/Doctorale/Project/AMPBAN/data/independent_test_data/MFA_independent_stru.pt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e8638f85-4fb0-415b-a9f6-af8669015868",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d53bf3236de4f748a109afa5f304bcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modeling_esm_plusplus.py: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/Synthyra/ESMplusplus_small:\n",
      "- modeling_esm_plusplus.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "Embedding Sequences: 100%|████████████████████████████████████████████████████████████████| 4/4 [00:08<00:00,  2.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings saved to: /mnt/e/Doctorale/Project/PlantAMP/plant_amp/amp_test_pos.pth\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Third step: Extract sequence feature\n",
    "import torch\n",
    "from transformers import AutoModelForMaskedLM\n",
    "from Bio import SeqIO\n",
    "import os\n",
    "from tqdm import tqdm  # For progress bars\n",
    "\n",
    "def embed_sequences(fasta_file, output_path, model_name='./script/Synthyra/ESMplusplus_small', batch_size=128, max_len=512,\n",
    "                    pooling_types=['mean', 'cls'], num_workers=4, use_gpu=True):\n",
    "    \"\"\"\n",
    "    Embeds protein sequences from a FASTA file using an ESM model and saves the embeddings.\n",
    "\n",
    "    Args:\n",
    "        fasta_file (str): Path to the FASTA file containing protein sequences.\n",
    "        output_path (str): Path to save the embeddings as a .pth file.\n",
    "        model_name (str, optional): Name of the ESM model to use.\n",
    "            Defaults to \"ESMplusplus_small\".\n",
    "        batch_size (int, optional): Batch size for embedding.\n",
    "            Adjust based on your GPU memory. Defaults to 2.\n",
    "        max_len (int, optional): Maximum sequence length.\n",
    "            Sequences longer than this will be truncated. Defaults to 512.\n",
    "        pooling_types (list, optional): Types of pooling to apply.\n",
    "            Defaults to ['mean', 'cls'].\n",
    "        num_workers (int, optional): Number of worker processes for data loading.\n",
    "            Defaults to 4.\n",
    "        use_gpu (bool, optional): Whether to use GPU if available. Defaults to True.\n",
    "    \"\"\"\n",
    "    # 1. Load the ESM model and tokenizer\n",
    "    try:\n",
    "        model = AutoModelForMaskedLM.from_pretrained(model_name, trust_remote_code=True)\n",
    "        tokenizer = model.tokenizer\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model: {e}\")\n",
    "        return\n",
    "\n",
    "    device = torch.device('cuda' if use_gpu and torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "\n",
    "    # 2. Read sequences from the FASTA file\n",
    "    try:\n",
    "        records = list(SeqIO.parse(fasta_file, \"fasta\"))\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading FASTA file: {e}\")\n",
    "        return\n",
    "    sequences = [str(record.seq) for record in records]\n",
    "    sequence_ids = [record.id for record in records] #get the sequence ids\n",
    "\n",
    "    # 3. Embed the sequences\n",
    "    embedding_dict = {}\n",
    "    with torch.no_grad():  # Disable gradient calculation for inference\n",
    "        for i in tqdm(range(0, len(sequences), batch_size), desc=\"Embedding Sequences\"):\n",
    "            batch_sequences = sequences[i:i + batch_size]\n",
    "            batch_ids = sequence_ids[i:i+batch_size] #get the ids for the current batch\n",
    "\n",
    "            # Tokenize the batch\n",
    "            inputs = tokenizer(batch_sequences, return_tensors=\"pt\", truncation=True, padding=True, max_length=max_len)\n",
    "            inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "\n",
    "            # Get the model outputs\n",
    "            outputs = model(**inputs)\n",
    "            # Extract the last hidden state\n",
    "            last_hidden_state = outputs.last_hidden_state\n",
    "\n",
    "            # 4. Apply pooling\n",
    "            pooled_embeddings = []\n",
    "            if 'mean' in pooling_types:\n",
    "                # Mean pooling (handle padding)\n",
    "                input_mask_expanded = inputs['attention_mask'].unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
    "                sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n",
    "                sum_mask = torch.sum(input_mask_expanded, 1)\n",
    "                mean_pooled = sum_embeddings / sum_mask\n",
    "                pooled_embeddings.append(mean_pooled)\n",
    "            if 'cls' in pooling_types:\n",
    "                # CLS token pooling\n",
    "                cls_pooled = last_hidden_state[:, 0]\n",
    "                pooled_embeddings.append(cls_pooled)\n",
    "\n",
    "            # Concatenate pooling results\n",
    "            if len(pooled_embeddings) > 1:\n",
    "                batch_embeddings = torch.cat(pooled_embeddings, dim=1)\n",
    "            else:\n",
    "                batch_embeddings = pooled_embeddings[0]\n",
    "\n",
    "            # Convert to the desired dtype\n",
    "            batch_embeddings = batch_embeddings.to(torch.float32).cpu()  # Move to CPU before saving\n",
    "\n",
    "            # Store the embeddings in the dictionary, use sequence IDs as keys\n",
    "            for j, seq_id in enumerate(batch_ids):\n",
    "                embedding_dict[seq_id] = batch_embeddings[j]\n",
    "\n",
    "    # 5. Save the embeddings\n",
    "    try:\n",
    "        torch.save(embedding_dict, output_path)\n",
    "        print(f\"Embeddings saved to: {output_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving embeddings: {e}\")\n",
    "        return\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage:\n",
    "    fasta_file = \"/mnt/e/Doctorale/Project/PlantAMP/plant_amp/amp_test_pos.fa\"  # Replace with your FASTA file\n",
    "    output_path = \"/mnt/e/Doctorale/Project/PlantAMP/plant_amp/amp_test_pos.pth\"  # Replace with your desired output path\n",
    "    embed_sequences(fasta_file, output_path)\n",
    "    print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503441c0-5fd4-430d-bb38-0f792da11954",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The sequence embedding of training data and test data were below\n",
    "/mnt/e/Doctorale/Project/AMPBAN/data/training_pos_aug.pth\n",
    "/mnt/e/Doctorale/Project/AMPBAN/data/training_neg_aug.pth\n",
    "/mnt/e/Doctorale/Project/AMPBAN/data/independent_test_data/xu_embeddings_by_id.pth\n",
    "/mnt/e/Doctorale/Project/AMPBAN/data/independent_test_data/Xiao_embeddings_by_id.pth\n",
    "/mnt/e/Doctorale/Project/AMPBAN/data/independent_test_data/MFA_embeddings_by_id.pth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47858c28-e845-4d96-a9ae-0aaa1d717aeb",
   "metadata": {},
   "source": [
    "### Training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031bf9e2-3678-428f-acb7-f5d13ccd3600",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
